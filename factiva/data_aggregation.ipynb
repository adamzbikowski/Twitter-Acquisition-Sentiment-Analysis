{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['DATE', 'OUTLET', 'AUTHOR', 'DESCRIPTION', 'TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.read_csv(\"Data/Youtube Comments.csv\")\n",
    "youtube['OUTLET'] = \"YouTube\"\n",
    "youtube['DESCRIPTION'] = pd.NA\n",
    "youtube.drop('video_id', axis=1, inplace=True)\n",
    "youtube.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "youtube = youtube[col_order]\n",
    "\n",
    "youtube['DATE'] = pd.to_datetime(youtube['DATE'], errors='coerce')\n",
    "youtube['DATE'] = youtube['DATE'].dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing = pd.read_csv(\"Data/bing_data.csv\")\n",
    "bing.drop('ARTICLE NUMBER', axis=1, inplace=True)\n",
    "bing['OUTLET'] = bing['AUTHOR']\n",
    "bing['AUTHOR'] = pd.NA\n",
    "bing.drop('URL', axis=1, inplace=True)\n",
    "\n",
    "bing = bing[col_order]\n",
    "bing['DATE'] = pd.to_datetime(bing['DATE'], errors='coerce')\n",
    "bing['DATE'] = bing['DATE'].dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = pd.read_csv(\"Data/google_data.csv\")\n",
    "google['URL'] = google['URL'].str.replace('https://', '')\n",
    "google['URL'] = google['URL'].str.replace('www.', '')\n",
    "google['URL'] = google['URL'].str.replace(r'\\.c.*', '', regex=True)\n",
    "google['URL'] = google['URL'].str.replace(r'\\.e.*', '', regex=True)\n",
    "google['URL'] = google['URL'].str.replace(r'\\.o.*', '', regex=True)\n",
    "google['URL'] = google['URL'].str.replace(r'\\.g.*', '', regex=True)\n",
    "google['URL'] = google['URL'].str.replace(r'\\.f.*', '', regex=True)\n",
    "google['URL'] = google['URL'].str.replace(r'\\.i.*', '', regex=True)\n",
    "\n",
    "google['OUTLET'] = google['URL']\n",
    "google['AUTHOR'] = pd.NA\n",
    "\n",
    "google.drop('URL', axis=1, inplace=True)\n",
    "google.drop('Title', axis=1, inplace=True)\n",
    "\n",
    "col_names = ['DATE', 'DESCRIPTION', 'TEXT', 'OUTLET', 'AUTHOR' ]\n",
    "google.columns = col_names\n",
    "\n",
    "google = google[col_order]\n",
    "\n",
    "google['DATE'] = pd.to_numeric(google['DATE'], errors='coerce')\n",
    "google.dropna(subset=['DATE'], inplace=True)\n",
    "\n",
    "google['DATE'] = pd.to_datetime(google['DATE'], errors='coerce')\n",
    "google['DATE'] = google['DATE'].dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "factivaA = pd.read_csv(\"Factiva.csv\")\n",
    "factivaA.drop('ARTICLE_NUMBER', axis=1, inplace=True)\n",
    "factivaA.drop('ERRORS', axis=1, inplace=True)\n",
    "factivaA['DESCRIPTION'] = pd.NA\n",
    "\n",
    "factivaA = factivaA[col_order]\n",
    "\n",
    "factivaA['DATE'] = pd.to_datetime(factivaA['DATE'], errors='coerce')\n",
    "factivaA['DATE'] = factivaA['DATE'].dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date_index(lst):\n",
    "    for index, item in enumerate(lst):\n",
    "        try:\n",
    "            datetime.strptime(item, '%d %B %Y')\n",
    "            return index \n",
    "        except ValueError:\n",
    "            pass \n",
    "    return None \n",
    "\n",
    "def cleaner(file_content, code, outlet):\n",
    "\n",
    "    articles_list = file_content.split('\\nDocument ' + code)\n",
    "\n",
    "    texts = []\n",
    "    authors = []\n",
    "    dates = []  \n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for element in articles_list:\n",
    "        \n",
    "        elem_list = element.split('\\n')\n",
    "\n",
    "        if counter > 0:\n",
    "            try:\n",
    "                elem_list = elem_list[3:]\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "        date_loc = find_date_index(elem_list)\n",
    "\n",
    "        if not date_loc is None:\n",
    "\n",
    "            temp_author = elem_list[date_loc-2]\n",
    "            authors.append(temp_author)\n",
    "\n",
    "            temp_date = elem_list[date_loc]\n",
    "            dates.append(temp_date)\n",
    "\n",
    "            temp_text = '\\n'.join(elem_list[date_loc+6:])\n",
    "            texts.append(temp_text)\n",
    "    \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        final_df = pd.DataFrame({'DATE': dates, 'OUTLET': outlet, 'AUTHOR': authors, 'DESCRIPTION': pd.NA, 'TEXT': texts})\n",
    "\n",
    "        final_df = final_df[col_order]\n",
    "\n",
    "        final_df['DATE'] = pd.to_datetime(final_df['DATE'], format='%d %B %Y', errors='coerce')\n",
    "\n",
    "        final_df.dropna(subset=['DATE'], inplace=True)\n",
    "\n",
    "        final_df['DATE'] = final_df['DATE'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "\n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files_to_clean = [('b_i_articles_cleaned.txt','BI','business insider'),\n",
    "                      ('barrons_articles_cleaned.txt','BON','barrons'),\n",
    "                      ('the_guardian_articles_cleaned.txt','GRD','guardian'),\n",
    "                      ('w_p_articles_cleaned.txt','WP','washington post')]\n",
    "\n",
    "factivaAlex = pd.DataFrame(columns=factivaA.columns)\n",
    "\n",
    "for file_info in txt_files_to_clean:\n",
    "\n",
    "    with open('Data/'+ file_info[0], 'r') as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    temp_df = cleaner(file_content, file_info[1], file_info[2])\n",
    "\n",
    "    factivaAlex = pd.concat([factivaAlex, temp_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "factivaAlex.to_csv(\"Factiva_Alex.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleData = pd.concat([youtube, bing, google, factivaA, factivaAlex], ignore_index=True)\n",
    "ArticleData.to_csv(\"ArticleData.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StandardKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
